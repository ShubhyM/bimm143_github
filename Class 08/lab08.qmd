---
title: "Class 8: PCA Mini Project"
author: "Shubhayan Manjrekar: A17128282"
format: gfm
---


It is important to consider scalling your data before analysis such as PCA. 

For example:

```{r}
head(mtcars)
```

```{r}
apply(mtcars, 2, sd)
```



```{r}
x<- scale(mtcars)
head(x)
```



```{r}
round(colMeans(x),2)
```


Key-point: It is usually always a good idea to scale your data before to PCA...


```{r}
fna.data <- "WisconsinCancer.csv"
wisc.df <- read.csv(fna.data, row.names=1)
head(wisc.df)
```


```{r}
diagnosis<-wisc.df[,1]
table(diagnosis)
```


essentially the expert answer that we will compare our analysis results to:

```{r}
# We can use -1 here to remove the first column
wisc.data<- wisc.df[,-1]
head(wisc.data)
```


```{r}
diagnosis<-c(diagnosis)
```


## Exploratory data analysis

> Q1. How many observations are in this dataset?


```{r}
num_observations <- length(rownames(data))
print(num_observations)
```

> Q2. How many of the observations have a malignant diagnosis?


```{r}

```


> Q3. How many variables/features in the data are suffixed with _mean?



```{r}

length(grep("_mean", colnames(wisc.data)) )
```


## Principal Component Analysis

```{r}
wisc.pr<-prcomp(wisc.data, scale=T)
summary(wisc.pr)
```




Main "PC score plot", "PC1 vs PC2 plot"


See what is in our PCA result object:


```{r}
attributes(wisc.pr)
```


```{r}
head
```


```{r}
wisc.pr$x
#plot(wisc.pr$x)
```







```{r}
plot(wisc.pr$x[,1],wisc.pr$x[,2], 
     col=as.factor(diagnosis))
```



```{r}
pca_result <- prcomp(wisc.data, scale = TRUE)

#create a biplot
biplot(pca_result)

#something that stands out are the overlapping in the plot. It is hard to understand because there is no visible relationship in the plot.
```


>Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

Each dot represents a patient.It is hard to understand because there is no visible relationship in the plot.


> Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?


> Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?


> Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

>Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

> Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?

```{r}
plot(wisc.pr$x[,1],wisc.pr$x[,3], 
     col=as.factor(diagnosis))
```

These plots are quite similar but show a different principal component.


> Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?


```{r}
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis

library(ggplot2)

ggplot(df) + 
  aes(PC1, PC2, col = diagnosis) +  # Use 'diagnosis' to color points
  geom_point() 
```




> Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?




> Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?





>Q12. Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10?






>Q13. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.








```{r}
x<- summary(wisc.pr)
plot(x$importance[2,], typ="b")
```



## Clustering

```{r}
d<- dist(wisc.data)
hc.raw<-hclust(d)
plot(hc.raw)
```




## Combine PCA and clustering

Our PCA results were in `wisc.pr$x` 

```{r}
#distance matrix from PCA result
d <-dist(wisc.pr$x[,1:3])
hc<- hclust(d, method="ward.D2")
plot(hc)
```

Cutree into two groups/branches/clusters...

```{r}
grps<- cutree(hc, k=2)
```

```{r}
plot(wisc.pr$x, col=grps)
```




Compare my clustering result(my `grps`) to the expert `diagnosis`

```{r}
table(diagnosis)
table(grps)
```


```{r}
table(diagnosis,grps)
```

B= benign
M=malignant




Individual 2.
